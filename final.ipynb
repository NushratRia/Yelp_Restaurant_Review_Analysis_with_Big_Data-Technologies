{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in ./venv/lib/python3.12/site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in ./venv/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask[complete] in ./venv/lib/python3.12/site-packages (2024.9.1)\n",
      "Requirement already satisfied: click>=8.1 in ./venv/lib/python3.12/site-packages (from dask[complete]) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in ./venv/lib/python3.12/site-packages (from dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in ./venv/lib/python3.12/site-packages (from dask[complete]) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from dask[complete]) (24.1)\n",
      "Requirement already satisfied: partd>=1.4.0 in ./venv/lib/python3.12/site-packages (from dask[complete]) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in ./venv/lib/python3.12/site-packages (from dask[complete]) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in ./venv/lib/python3.12/site-packages (from dask[complete]) (1.0.0)\n",
      "Requirement already satisfied: pyarrow>=14.0.1 in ./venv/lib/python3.12/site-packages (from dask[complete]) (17.0.0)\n",
      "Requirement already satisfied: lz4>=4.3.2 in ./venv/lib/python3.12/site-packages (from dask[complete]) (4.3.3)\n",
      "Requirement already satisfied: locket in ./venv/lib/python3.12/site-packages (from partd>=1.4.0->dask[complete]) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in ./venv/lib/python3.12/site-packages (from pyarrow>=14.0.1->dask[complete]) (2.1.2)\n",
      "Requirement already satisfied: pandas>=2.0 in ./venv/lib/python3.12/site-packages (from dask[complete]) (2.2.3)\n",
      "Requirement already satisfied: dask-expr<1.2,>=1.1 in ./venv/lib/python3.12/site-packages (from dask[complete]) (1.1.15)\n",
      "Requirement already satisfied: bokeh>=3.1.0 in ./venv/lib/python3.12/site-packages (from dask[complete]) (3.6.0)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in ./venv/lib/python3.12/site-packages (from dask[complete]) (3.1.4)\n",
      "Requirement already satisfied: distributed==2024.9.1 in ./venv/lib/python3.12/site-packages (from dask[complete]) (2024.9.1)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in ./venv/lib/python3.12/site-packages (from distributed==2024.9.1->dask[complete]) (1.1.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in ./venv/lib/python3.12/site-packages (from distributed==2024.9.1->dask[complete]) (6.0.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in ./venv/lib/python3.12/site-packages (from distributed==2024.9.1->dask[complete]) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in ./venv/lib/python3.12/site-packages (from distributed==2024.9.1->dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.2.0 in ./venv/lib/python3.12/site-packages (from distributed==2024.9.1->dask[complete]) (6.4.1)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in ./venv/lib/python3.12/site-packages (from distributed==2024.9.1->dask[complete]) (2.2.3)\n",
      "Requirement already satisfied: zict>=3.0.0 in ./venv/lib/python3.12/site-packages (from distributed==2024.9.1->dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: contourpy>=1.2 in ./venv/lib/python3.12/site-packages (from bokeh>=3.1.0->dask[complete]) (1.3.0)\n",
      "Requirement already satisfied: pillow>=7.1.0 in ./venv/lib/python3.12/site-packages (from bokeh>=3.1.0->dask[complete]) (10.4.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in ./venv/lib/python3.12/site-packages (from bokeh>=3.1.0->dask[complete]) (2024.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2>=2.10.3->dask[complete]) (3.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas>=2.0->dask[complete]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas>=2.0->dask[complete]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas>=2.0->dask[complete]) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[complete]) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"dask[complete]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/12 13:28:59 WARN Utils: Your hostname, Nushrats-Laptop.local resolves to a loopback address: 127.0.0.1; using 192.168.0.81 instead (on interface en0)\n",
      "24/10/12 13:28:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/12 13:28:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/10/12 13:29:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|origin|     avg_dep_delay|\n",
      "+------+------------------+\n",
      "|   SFO|21.640471512770137|\n",
      "|   IAH|19.142857142857142|\n",
      "|   EWR|13.450171821305842|\n",
      "|   MIA|11.569018404907975|\n",
      "|   ATL|10.623355263157896|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Flight Data Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "flights_df = spark.read.json(\"flights.json\")\n",
    "aa_flights_df = flights_df.filter(flights_df.carrier == \"AA\")\n",
    "\n",
    "\n",
    "avg_delay_df = aa_flights_df.groupBy(\"origin\").agg(avg(\"depdelay\").alias(\"avg_dep_delay\"))\n",
    "\n",
    "\n",
    "sorted_avg_delay_df = avg_delay_df.orderBy(\"avg_dep_delay\", ascending=False)\n",
    "\n",
    "top_5_origin_airports = sorted_avg_delay_df.limit(5)\n",
    "\n",
    "top_5_origin_airports.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The origin airport with the 2nd worst average departure delay is: IAH with an average delay of 19.142857142857142 minutes.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Flight Data Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the JSON data into a DataFrame\n",
    "flights_df = spark.read.json(\"flights.json\")\n",
    "\n",
    "# Filter for American Airlines flights (carrier \"AA\")\n",
    "aa_flights_df = flights_df.filter(flights_df.carrier == \"AA\")\n",
    "\n",
    "# Group by origin and calculate the average departure delay\n",
    "avg_delay_df = aa_flights_df.groupBy(\"origin\").agg(avg(\"depdelay\").alias(\"avg_dep_delay\"))\n",
    "\n",
    "# Sort by average departure delay in descending order\n",
    "sorted_avg_delay_df = avg_delay_df.orderBy(\"avg_dep_delay\", ascending=False)\n",
    "\n",
    "# Get the 2nd origin airport with the worst (longest) average departure delay\n",
    "second_worst_origin_airport = sorted_avg_delay_df.collect()[1]\n",
    "\n",
    "# Print the results\n",
    "print(f\"The origin airport with the 2nd worst average departure delay is: {second_worst_origin_airport['origin']} \"\n",
    "      f\"with an average delay of {second_worst_origin_airport['avg_dep_delay']} minutes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The worst delayed airport: SFO \n",
      "The 2nd worst delayed airport: IAH \n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg as a\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Flight Delay\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "flights_df = spark.read.json(\"flights.json\")\n",
    "aa_flights_df = flights_df.filter(flights_df.carrier == \"AA\")\n",
    "avg_delay_df = aa_flights_df.groupBy(\"origin\").agg(a(\"depdelay\").alias(\"avg_dep_delay\"))\n",
    "sorted_avg_delay_df = avg_delay_df.orderBy(\"avg_dep_delay\", ascending=False)\n",
    "\n",
    "worst_origin_airport = sorted_avg_delay_df.collect()[0]\n",
    "print(f\"The worst delayed airport: {worst_origin_airport['origin']} \")\n",
    "\n",
    "second_worst_origin_airport = sorted_avg_delay_df.collect()[1]\n",
    "print(f\"The 2nd worst delayed airport: {second_worst_origin_airport['origin']} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/12 13:48:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The worst delayed airport: SFO \n",
      "The 2nd worst delayed airport: IAH \n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg as a\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Flight Delay\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "flights_df = spark.read.json(\"flights.json\")\n",
    "aa_flights_df = flights_df.filter(flights_df.carrier == \"AA\")\n",
    "avg_delay_df = aa_flights_df.groupBy(\"origin\").agg(a(\"depdelay\").alias(\"avg_dep_delay\"))\n",
    "sorted_avg_delay_df = avg_delay_df.orderBy(\"avg_dep_delay\", ascending=False)\n",
    "sorted_results = sorted_avg_delay_df.collect()\n",
    "\n",
    "if len(sorted_results) >= 2:\n",
    "    worst_origin_airport = sorted_results[0]\n",
    "    print(f\"The worst delayed airport: {worst_origin_airport['origin']} \")\n",
    "\n",
    "    second_worst_origin_airport = sorted_results[1]\n",
    "    print(f\"The 2nd worst delayed airport: {second_worst_origin_airport['origin']} \")\n",
    "else:\n",
    "    print(\"Not enough data\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
